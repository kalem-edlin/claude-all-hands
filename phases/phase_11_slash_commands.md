# Phase 11: Slash Commands

## Objective
Create slash command definitions that orchestrate the full planning and implementation workflows.

## Scope
- /plan, /continue, /whats-next, /debug
- /create-specialist, /create-skill
- /audit-docs, /create-docs
- Knowledge banks (feature, debugging, suggestions)

## Implementation Details

### Command File Location
Command files live in `.claude/commands/<command_name>.md`

---

## Command Definitions

### /plan [user prompt] [--quick | --create | --refine]

#### --quick mode
* No additional questions
* No delegation -> Main agent makes inference itself
* Call `envoy plan check` to determine context:
    * If plan exists (status = in_progress or completed):
        * Add user prompt to the user_input.md branch
        * Call planning agent to modify plan to include minimal prompts on top of everything (either at end if plan completed, or against most relevant prompt file and will only depend on tasks that are completed -> meaning it will be implemented on NEXT with a --quick modifier
        * Final user gate like the original planning workflow to start implementation
        * Pass back to main agent and implement regularly where /continue supports
    * If no plan exists:
        * Create a new branch for autogenerated plan file structure
        * Add user prompt to the user_input.md branch
        * Call planning agent to create a very simple plan to include a simple prompt set with a --quick modifier to planner
        * Final user gate like the original planning workflow to start implementation
        * Pass back to main agent and implement regularly where /continue supports

#### Standard mode (--create | --refine)
1. Accept User Initial Prompt for the feature / amendment to existing plan
2. Call `envoy plan check`
    1. If no plan: but user_input.md contains answers, continue to top-level step 3 after reading user_input.md to ask any more required questions
    2. If no plan: Create new branch automatically inferred from user prompt --create mode from this point forward
    3. If plan and prompt is not relevant / complimentary: Ask user if they wish to continue
        1. If yes: set to --refine mode
        2. If no: create new branch off of base branch
    4. If plan and prompt is relevant: set to --refine mode
3. Input Gate:
    1. Ask input gate questions that CANNOT be inferred from initial prompt -> if asked, each can be skipped to be automatically inferred based on wider context
    2. Only questions that are relevant to the initial prompt when in --refine mode are asked
    3. Use the progressive disclosure approach for input gate questions starting with the following:
        1. What type of change is this - UI / Frontend - Backend / API - Full-stack - Observability / Monitoring - Developer Experience (DX) - Infrastructure / DevOps - Other (describe) (only if not already inferred)
        2. Any additional constraints? (always ask this)
        3. Then allow the user to continue (for easy tasks, or receive more questions from the LLM inferred from information banks that its recommended for the LLM to know that the LLM will build questions from (ask 3, continue with current context)
        4. Refer to recommended feature knowledge bank for input gate to build these questions from
4. Tell user that if there is a UX feature involved that they can add UX screenshots with context labels to the filesystem
    1. Ask a user the question of if would like to include any UI screenshots / design material in the manifest
    2. We will need a new design file with a manifest to describe screenshots and what tasks / responsibilities they are used for
    3. These will be added to context of discovery (via delegation) and implementation (via prompt approach reference) agents
    4. Main agent reads this and is aware of the manifest information for delegation (use envoy to get context of design resources without actually reading the file)
5. Write all user generated context, including initial user prompt to user_input.md file using `envoy plan append-user-input "<content>"`
6. Specialist Delegation
    1. Internally extract Atomic Requirements from user prompt + answered questions and cluster by primary domain
    2. If confidence on a given segment's assignment to specialist is lacking:
        1. Ask user if they want to /create-specialist before continuing
    3. If no specialist found and create specialist not run: choose "surveyor" agent instead
    4. For each segment, delegate to appropriate specialist:
        * "Run `envoy protocol discovery` and follow the steps. INPUTS: `{ agent_name: <specialist_name>[_N], segment_context: <requirements_for_segment> }`"
        * If multiple segments fit the same specialist: add {_N} suffix to agent_name
        * OUTPUTS: `{ success: true }`
7. Call `envoy plan get-findings` to get a list of all approaches (to determine research delegation needs)
8. Research Delegation
    1. For each distinct research objective, delegate to **researcher agent**:
        * "Run `envoy protocol discovery` and follow the steps. INPUTS: `{ agent_name: "researcher"[_N], segment_context: <research_objectives_with_approach_references> }`"
        * If multiple research agents needed: add {_N} suffix to agent_name
        * OUTPUTS: `{ success: true }`
9. Findings Gate:
    1. Ask user all clarifying questions from approach documents
    2. Ask user if they want top-level redirection (hard-reset delegation with specific requirements)?
        1. If chosen: clear all findings files, return to step 6
    3. Call `envoy plan block-findings-gate`
        * Returns: { thoughts, affected_approaches: [{ specialist_name, approach_number }] }
    4. If any affected_approaches returned:
        1. Re-delegate to affected specialists:
            * "Run `envoy protocol discovery` and follow the steps. INPUTS: `{ agent_name: <specialist_name>, segment_context: <thoughts>, approach_references: [{ specialist_name, approach_num }] }`"
        2. Rerun findings gate before proceeding
10. Planner Delegation:
    * Delegate to **planner agent** with **planning-workflow**
    * INPUTS: `{ mode: "create" | "refine", workflow_type: "feature", feature_branch: <current_branch> }`
    * OUTPUTS: `{ success: true }`
11. Call /continue command

---

### /continue
1. Call `envoy plan next [-n <count>]` to get next prompts (if count > 1 then tasks returned are independent with no dependencies)
    1. If the prompt is a variant, must return ALL variants to be run in parallel (done regardless of count)
2. Call to next retrieves:
    1. Description
    2. Relevant file list
3. Delegate to specialist
    1. For each next prompt, delegate to appropriate specialist:
        * If prompt is debug: "Run `envoy protocol debugging` and follow the steps. INPUTS: `{ prompt_num: <N>, variant: <V>, feature_branch: <current_branch> }`"
        * Otherwise: "Run `envoy protocol implementation` and follow the steps. INPUTS: `{ prompt_num: <N>, variant: <V>, feature_branch: <current_branch> }`"
    2. If no suitable specialist found:
        1. Ask user if they would like to /create-specialist to fill the need of this task
        2. else: choose "worker" agent instead
4. After specialist returns (prompt merged), delegate to documentor for that prompt:
    * Delegate to **documentor agent** with **extract-workflow**
    * INPUTS: `{ mode: "extract", prompt_num: <N>, variant: <V>, feature_branch: <current_branch> }`
    * OUTPUTS: `{ success: true }`
5. Rerun loop (steps 1-4) after each prompt+documentation cycle until no more prompts and no prompts in progress
6. Call `envoy gemini review --full`
    * Returns: { verdict, thoughts?, answered_questions?, suggested_fixes? }
    * Full review examines all prompts, curator.md, user_input.md against feature branch git diff
7. If full feature review fails (verdict = failed or suggested_fixes exist):
    1. Delegate to relevant specialists to implement suggested_fixes, passing `thoughts` and answered_questions as context
    2. Commit changes and rerun full review until passes
8. Mandatory documentation auditor (single documentor in audit mode):
    * Delegate to **documentor agent** with **audit-workflow**
    * INPUTS: `{ mode: "audit", feature_branch: <current_branch> }`
    * OUTPUTS: `{ success: true }`
9. Pre-PR Review - parallel agents (Phase 14):
    * Delegate in PARALLEL:
        a) **curator agent** with **pre-pr-review** workflow
           * INPUTS: `{ feature_branch: <current_branch> }`
           * OUTPUTS: `{ recommendations: [...], has_changes: boolean }`
        b) **code-simplifier agent** with **simplification-review** workflow
           * INPUTS: `{ feature_branch: <current_branch> }`
           * OUTPUTS: `{ simplifications: [...], has_changes: boolean }`
    * If any changes: user decides (I)mplement / (D)efer / (S)kip per item
    * If implement: respective agent makes changes, commit before PR
10. Call `envoy plan complete` to generate summary, create PR, and mark plan as completed
11. Call /whats-next command

---

### /whats-next
1. Call `envoy plan check`
    * Returns: { status, context based on status }
    * If in_progress: tell user to run /plan --refine or /continue, then break thread
    * If completed: continue with returned summary context
2. Use returned context (user_input.md, summary.md, plan.md, prompt descriptions) for suggestions
3. Using the domains below, generate **3-5 concrete suggestions** for what could come next. Prioritize based on: 1. Explicitly mentioned follow-ups from user_input.md 2. Natural extensions of what was built 3. Quality/hardening improvements 4. New capabilities enabled by what exists
4. **Ready to continue?** - `/plan {suggestion}` - Start planning one of these - `/plan {your own idea}` - Plan something else - `/plan --refine` - Add to the current plan instead of starting fresh - `/debug {issue}` - If you found a bug to fix first

---

### /debug [user bug description] [--create | --refine | --quick]

#### --quick mode
* No additional questions
* No specialist delegation -> Main agent makes inference itself
* Creates no observability prompt because that question was not asked
1. Call `envoy plan check` to determine context
2. If no plan exists: Create new branch automatically inferred from bug description
3. Add user bug description to the user_input.md file using `envoy plan append-user-input "<content>"`
4. Planner Delegation (Quick Debug Plan):
    * Delegate to **planner agent** with **planning-workflow**
    * INPUTS: `{ mode: "quick", workflow_type: "debug", feature_branch: <current_branch>, plan_status: <status_from_check> }`
    * OUTPUTS: `{ success: true }`
5. Call /continue command

#### --create / --refine mode
1. Accept User Initial Bug Description (symptoms, error messages, reproduction steps, expected vs actual behavior)
2. Call `envoy plan check`
    1. If no plan: but user_input.md contains answers, continue to top-level step 3 after reading user_input.md to ask any more required questions
    2. If no plan: Create new branch automatically inferred from bug description --create mode from this point forward
    3. If plan and bug is unrelated to current work: Ask user if they wish to continue
        1. If yes: set to --refine mode
        2. If no: create new branch off of base branch, set to --create mode
    4. If plan and bug is related to current work (potentially introduced by current plan): set to --refine mode
3. Input Gate:
    1. Ask input gate questions that CANNOT be inferred from initial bug description -> if asked, each can be skipped to be automatically inferred based on wider context
    2. Only questions that are relevant to the bug context when in --refine mode are asked
    3. Use the progressive disclosure approach for input gate questions starting with the following:
        1. Asks a specific question of how we would like to manage observability for this bug once identified for future safety (always ask - critical for debug workflow)
        2. Can you share the exact error message and the steps to reproduce it? (if not already provided)
        3. When did this start happening? (recent deploy, always, after specific change)
        4. Any suspected area of code or recent changes to related areas? (if not already inferred)
        5. What is the severity / urgency? (blocks release, can workaround, nice to fix)
        6. Then allow the user to continue (for easy tasks), or receive more questions from the LLM inferred from information banks that its recommended for the LLM to know (ask 3 questions, continue with current context)
        7. Refer to recommended debugging knowledge bank for input gate to build these questions from
4. Write all user generated context, including initial bug description to user_input.md file using `envoy plan append-user-input "<content>"`
5. Specialist Delegation (Bug Discovery)
    1. Internally extract bug symptoms, suspected areas, and reproduction context and cluster by primary domain
    2. If confidence on a given segment's assignment to specialist is lacking:
        1. Ask user if they want to /create-specialist before continuing
    3. If no specialist found and create specialist not run: choose "surveyor" agent instead
    4. For each segment, delegate to appropriate specialist:
        * "Run `envoy protocol bug-discovery` and follow the steps. INPUTS: `{ agent_name: <specialist_name>[_N], segment_context: <bug_context_for_segment> }`"
        * If multiple segments fit the same specialist: add {_N} suffix to agent_name
        * OUTPUTS: `{ success: true }`
6. Call `envoy plan get-findings` to get a list of all bug hypotheses/approaches (to determine research delegation needs)
7. Research Delegation (Bug Research)
    1. For each distinct research objective, delegate to **researcher agent**:
        * "Run `envoy protocol bug-discovery` and follow the steps. INPUTS: `{ agent_name: "researcher"[_N], segment_context: <research_objectives_with_approach_references> }`"
        * Research objectives should focus on: known issues in libraries/frameworks, similar error messages, anti-patterns
        * If multiple research agents needed: add {_N} suffix to agent_name
        * OUTPUTS: `{ success: true }`
8. Findings Gate (Bug Hypotheses Review):
    1. Ask user all clarifying questions from approach documents (bug hypotheses)
    2. Ask user if they want top-level redirection (hard-reset delegation with specific requirements)?
        1. If chosen: clear all findings files, return to step 5
    3. Call `envoy plan block-findings-gate`
        * Returns: { thoughts, affected_approaches: [{ specialist_name, approach_number }] }
    4. If any affected_approaches returned:
        1. Re-delegate to affected specialists:
            * "Run `envoy protocol bug-discovery` and follow the steps. INPUTS: `{ agent_name: <specialist_name>, segment_context: <thoughts>, approach_references: [{ specialist_name, approach_num }] }`"
        2. Rerun findings gate before proceeding
9. Planner Delegation (Debug Plan):
    * Delegate to **planner agent** with **planning-workflow**
    * INPUTS: `{ mode: "create" | "refine", workflow_type: "debug", feature_branch: <current_branch> }`
    * OUTPUTS: `{ success: true }`
10. Call /continue command (implementation handled by debugging protocol for --debug prompts, implementation protocol for observability prompt)

---

### /create-specialist [initial context]
* Initial context = from manual user invocation or automatic call based on specialist need workflow call
* Create a curator/ branch on top of current branch
* Will not follow any planning material
* Input gate
    * Responsibility of agent (which will infer what skills it uses)
    * Area of expertise (code base files / areas)
    * Specific skills it should have access to that exist
    * Any new skills that the user requires be implemented and URLs it should store as references / core to skill
    * Claude envoy usage (what claude envoy commands should it use - must run envoy help command to show all commands available)
* Delegate to **curator agent** with **curation-workflow**
    * INPUTS: `{ mode: "create", artifact_type: "specialist", initial_context: <input_gate_summary> }`
    * OUTPUTS: `{ success: true, clarifying_questions?: [string] }`
* If clarifying_questions returned: present to user, then continue
* Commit changes
* Delegate to **curator agent** with **curation-audit-workflow**
    * INPUTS: `{ mode: "audit", branch_name: <current_branch> }`
    * OUTPUTS: `{ success: true, amendments_made: boolean }`
* Ask user how they wish to test the agent
* Ask user to mark when completed testing (any feedback or good to merge)
* Commit changes
    * If parent branch is not base branch and had plan matter: merge back, add updates to curator.md
    * If parent branch is base branch (check via `envoy git is-base-branch`): call `envoy git create-pr --title "<title>" --body "<body>"`

---

### /create-skill [user prompt]
* Initial context = from manual user invocation or automatic call based on specialist need workflow call
* Create a curator/ branch on top of current branch (with inferred name)
* Input gate
    * Goals of this skill
    * Which agents will use this (important for understanding its use case)
    * URLs it should store as references / core to skill
    * Directory to serve as documentation for
* Delegate to **curator agent** with **curation-workflow**
    * INPUTS: `{ mode: "create", artifact_type: "skill", initial_context: <input_gate_summary> }`
    * OUTPUTS: `{ success: true, clarifying_questions?: [string] }`
* If clarifying_questions returned: present to user, then continue
* Commit changes
* Delegate to **curator agent** with **curation-audit-workflow**
    * INPUTS: `{ mode: "audit", branch_name: <current_branch> }`
    * OUTPUTS: `{ success: true, amendments_made: boolean }`
* Ask user how they wish to test the skill
* Ask user to mark when completed testing (any feedback or good to merge)
* Commit changes
    * If parent branch is not base branch and had plan matter: merge back, add updates to curator.md
    * If parent branch is base branch (check via `envoy git is-base-branch`): call `envoy git create-pr --title "<title>" --body "<body>"`

---

### /audit-docs [...documentation OR codebase directories and paths] [optional user prompt concerns]
* Initial context = from user prompt concerns or assumptions from directory and path names
* Must checkout base branch (via `envoy git checkout-base`)
* Create a docs/ branch on top of current branch (with inferred name)
* Main agent intelligently determines from file names how to break up tasks (and if tasks need to be broken up at all)
* Delegate to **documentor agent(s)** with **audit-workflow** (parallel if multiple)
    * INPUTS: `{ mode: "audit", feature_branch: <current_branch>, scope_paths: [<paths>], concerns: <user_concerns> }`
    * OUTPUTS: `{ success: true, findings: [...] }`
* User accepts findings OR provides extra context/direction
* Delegate single **documentor agent** with **audit-workflow** to implement all fix requirements and user decisions
    * INPUTS: `{ mode: "audit", feature_branch: <current_branch>, user_decisions: <decisions> }`
    * OUTPUTS: `{ success: true }`
* Commit changes, call `envoy git create-pr --title "<title>" --body "<body>"`

---

### /create-docs [...optional codebase directories and paths] [optional user prompt context]
* Initial context = from user prompt context or assumptions from directory and path names
* Must checkout base branch (via `envoy git checkout-base`)
* Create a docs/ branch on top of current branch (with inferred name)
* If directories and paths are not given, assume the whole codebase needs documentation
* Delegate to **documentor agent** with **coordination-workflow**
    * INPUTS: `{ mode: "coordinate", scope_paths: [<paths>] }`
    * OUTPUTS: `{ success: true, chunks: [{ paths, scope_description }] }`
* Delegate multiple **documentor agents** in parallel with **extract-workflow**
    * INPUTS: `{ mode: "create", feature_branch: <current_branch>, scope: <chunk> }`
    * OUTPUTS: `{ success: true }` per agent
* Delegate to **documentor agent** with **audit-workflow**
    * INPUTS: `{ mode: "audit", feature_branch: <current_branch> }`
    * OUTPUTS: `{ success: true, findings: [...] }`
* User accepts findings OR provides extra context/direction
* Delegate single **documentor agent** to implement all fix requirements and user decisions
* Commit changes, call `envoy git create-pr --title "<title>" --body "<body>"`

---

## Knowledge Banks

### Recommended Feature Knowledge Bank for Input Gate
```
**Generic (all types):** - Core mission / objective in one sentence - Success criteria (how do we know it's done?) - Explicit out-of-scope items - Acceptable level of jank / tech debt - Hard constraints (stack, APIs, security, existing patterns) - Primary users or systems affected - Key scenarios / user flows (1-3) - Happy path for each scenario - Must-do behaviors per scenario - Forbidden behaviors / invariants - Key edge cases - Anticipated follow-up work - Manual testing approach **UI / Frontend specific:** - UX quality bar (scrappy prototype | functional MVP | polished release) - Target devices / viewports - Accessibility requirements - Design references or screenshots available? - State management approach - Error / loading state handling **Backend / API specific:** - Input/output schemas (examples) - Performance targets (latency, throughput) - Data models / entities affected - Database migrations needed? - Authentication / authorization requirements - Rate limiting / quota considerations **Full-stack specific:** - All of the above - API contract between frontend and backend - Deployment coordination needs **Observability / Monitoring specific:** - What signals indicate success/failure? - Log levels and what to capture - Metrics to track - Alerting thresholds - Dashboard requirements **DX specific:** - Who is the developer audience? - Documentation requirements - Error message clarity - Local development impact - CI/CD pipeline changes **Infrastructure / DevOps specific:** - Environments affected (dev, staging, prod) - Rollback strategy - Security / compliance requirements - Scaling considerations - Cost implications **Informational context for LLM:** - The goal is confident delegation to specialists, not exhaustive documentation - Prioritize questions that would change the architectural approach - Questions about edge cases can often wait until findings gate - If the user's description is detailed, fewer questions are needed - Combine related bullets: "What's the success criteria, and how will you manually verify it works?" covers two bullets in one - Skip questions where the answer is obvious from codebase context - For --refine mode, only ask about NEW aspects not covered in existing plan
```

### Recommended Debugging Knowledge Bank for Input Gate
```
**Bug characterization:** - Exact observed behavior (error messages, incorrect output, crash) - Expected behavior - Reproduction steps (numbered) - Frequency (always, sometimes, specific conditions) - When it started (recent deploy, always, after specific change) - Environment details (device, browser, OS, app version, user type) **Investigation context:** - Any logs, errors, or stack traces already captured - Suspected area of code (if any) - Recent changes to related areas - Related features that DO work correctly - Data conditions that trigger it (specific user, specific input) **Constraints:** - Severity / urgency (blocks release, can workaround, nice to fix) - Areas of code that CANNOT be changed - Deadline pressure (quick patch vs proper fix) **Observability planning:** - How should we monitor for this bug recurring? - What signals would indicate the fix worked? - Any existing monitoring that should have caught this? **Informational context for LLM:** - Reproduction steps are critical - push for specifics - "When did it start" often reveals the cause - Stack traces / error messages are gold - ask if user has them - Frequency hints at race conditions vs logic errors - If user has a hypothesis, capture it even if uncertain - For --refine, only ask about NEW information - Combine related bullets: "Can you share the exact error message and the steps to reproduce it?" covers multiple bullets
```

### What's Next Suggestion Domains
```
**Directly Mentioned:** - Anything in "Anticipated follow-ups" from user_input.md - Items explicitly marked "out of scope for this iteration" - Technical debt the user said was acceptable for now **Natural Extensions:** - Additional user flows that build on implemented features - Edge cases that weren't covered but now matter - Performance optimizations for implemented features - Mobile/responsive versions if only desktop was built - API extensions if backend was built - Admin/management UI if user-facing was built **Quality & Hardening:** - Test coverage for implemented features - Error handling improvements - Accessibility enhancements - Documentation (README, API docs, inline comments) - Logging/monitoring for new features - Security hardening (input validation, auth edge cases) **Developer Experience:** - Local development improvements - CI/CD enhancements - Developer documentation - Code cleanup/refactoring opportunities identified during implementation **New Capabilities:** - Features that are now possible because of what was built - Integrations with other systems - Analytics/reporting on new functionality - User feedback mechanisms **Observability & Operations:** - Dashboards for new features - Alerting for failure modes - Performance baselines - Usage tracking
```

---

## Cross-Phase Context

### All Prior Phases
Commands orchestrate all envoy commands from Phases 1-8 and delegate to agents from Phase 10 using protocols from Phase 9.

### Curator Pre-PR (Phase 14)
Phase 14 defines the curator consultation step before PR creation. Curators review feature diffs for orchestration improvement opportunities (agents, skills, hooks, commands, protocols).

### Hooks (Phase 12)
Phase 12 will add enforcement hooks that:
* enforce_planning for user prompts done on base branch / without a plan
* Disable enforcement on quick/, docs/, curator/ branches

### Curator Updates Required
Update `.claude/agents/curator.md` with slash command orchestration patterns when implementing.

---

## Success Criteria
- [ ] /plan creates branch and delegates to specialists
- [ ] /plan --quick works with minimal questions
- [ ] /continue loops through prompts until complete
- [ ] /continue handles variants in parallel
- [ ] /whats-next generates relevant suggestions
- [ ] /debug follows debugging workflow
- [ ] /create-specialist creates curator branch and delegates
- [ ] /create-skill creates curator branch and delegates
- [ ] /audit-docs audits documentation
- [ ] /create-docs creates documentation
- [ ] Knowledge banks accessible to input gates
